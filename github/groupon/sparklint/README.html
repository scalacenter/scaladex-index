<div class="announce instapaper_body md" data-path="README.md" id="readme">
 <article class="markdown-body entry-content" itemprop="text">
  <h1><a id="user-content-sparklint" class="anchor" href="https://github.com/groupon/sparklint#sparklint" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Sparklint</h1> 
  <p>The missing Spark Performance Debugger that can be drag and dropped into your spark application!</p> 
  <h3><a id="user-content-mission" class="anchor" href="https://github.com/groupon/sparklint#mission" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Mission</h3> 
  <ul> 
   <li>Provide advance metrics and better visualization about your spark application's resource utilization 
    <ul> 
     <li>Vital application life time stats like idle time, average vcore usage, distribution of locality</li> 
     <li>View task allocation by executor</li> 
     <li>VCore usage graphs</li> 
     <li>(WIP) Find rdds that can benefit from persisting</li> 
     <li>(WIP) View task execution stats by locality</li> 
    </ul> </li> 
   <li>Help you find out where the bottle neck are 
    <ul> 
     <li>(WIP) Automated report on application bottle neck</li> 
    </ul> </li> 
   <li>(WIP) Opportunity to give the running application real-time hints on magic numbers like partitions size, job submission parallelism, whether to persist rdd or not</li> 
  </ul> 
  <p><a href="https://github.com/groupon/sparklint/blob/master/screen_shot.png" target="_blank"><img src="https://github.com/groupon/sparklint/raw/master/screen_shot.png" alt="ScreenShot" style="max-width:100%;"></a></p> 
  <h3><a id="user-content-usage" class="anchor" href="https://github.com/groupon/sparklint#usage" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Usage</h3> 
  <p>First figure out your current spark version. Please refer to <code>project/BuildUtils.scala</code> and look for <code>SUPPORTED_SPARK_VERSIONS</code>, these are the versions that we support out of box.</p> 
  <p>If your spark version has our precompiled sparklint jar (let's say 1.6.1 on scala 2.10), the jar name will be <code>sparklint-spark161_2.10</code>. Please note, in the jar the <code>161</code> means <code>Spark1.6.1</code> and <code>2.10</code> means <code>scala 2.10</code></p> 
  <p>If your spark version is not precompiled (i.e. 1.5.0), you can add an entry in <code>project/BuildUtils.getSparkMajorVersion</code>, then provide compatible code similar to spark-1.6 in <code>src/main/spark-1.5</code></p> 
  <p>For more detail about event logging, how to enable it, and how to gather log files, check <a href="http://spark.apache.org/docs/latest/configuration.html#spark-ui" target="_blank">http://spark.apache.org/docs/latest/configuration.html#spark-ui</a></p> 
  <h5><a id="user-content-live-mode-run-inside-spark-driver-node" class="anchor" href="https://github.com/groupon/sparklint#live-mode-run-inside-spark-driver-node" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Live mode (run inside spark driver node)</h5> 
  <p>SparklintListener is an implementation of <a href="https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/SparkFirehoseListener.html" target="_blank">SparkFirehoseListener</a> that listen to spark event log while the application is running. To enable it, you can try one of the following:</p> 
  <ol> 
   <li>Upload packaged jar to your cluster, include jar in classpath directly</li> 
   <li>Use <code>--packages</code> command to inject dependency during job submission if we have a precompiled jar, like <code>--conf spark.extraListeners=com.groupon.sparklint.SparklintListener --packages com.groupon.sparklint:sparklint-spark161_2.10:1.0.4</code></li> 
   <li>Add dependency directly in your pom, repackage your application, then during job submission, use <code>--conf spark.extraListeners=com.groupon.sparklint.SparklintListener</code></li> 
  </ol> 
  <p>Finally, find out your spark application's driver node address, open a browser and visit port 23763 (our default port) of the driver node.</p> 
  <blockquote> 
   <p>Add dependency directly for pom.xml</p> 
  </blockquote> 
  <pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.groupon.sparklint&lt;/groupId&gt;
    &lt;artifactId&gt;sparklint-spark161_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.0.4&lt;/version&gt;
&lt;/dependency&gt;
</code></pre> 
  <p>for build.sbt</p> 
  <pre><code>libraryDependencies += "com.groupon.sparklint" %% "sparklint-spark161" % "1.0.4"
</code></pre> 
  <h5><a id="user-content-server-mode-run-on-local-machine" class="anchor" href="https://github.com/groupon/sparklint#server-mode-run-on-local-machine" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Server mode (run on local machine)</h5> 
  <p>SparklintServer can run on your local machine. It will read spark event logs from the location specified. You can feed Sparklint an event log file to playback activities.</p> 
  <ul> 
   <li>Checking out the repo</li> 
   <li>Make sure you have <a href="http://www.scala-sbt.org/" target="_blank">SBT</a> installed.</li> 
   <li>Copy spark event log files to analyze into a directory then <code>sbt "run -d /path/to/log/dir -r"</code></li> 
   <li>Or analyze a single log file <code>sbt "run -f /path/to/logfile -r"</code></li> 
   <li>Then open browser and navigate to <code>http://localhost:23763</code></li> 
   <li>Spark version doesn't matter in server mode</li> 
   <li>Docker support available at <a href="https://hub.docker.com/r/roboxue/sparklint/" target="_blank">https://hub.docker.com/r/roboxue/sparklint/</a></li> 
  </ul> 
  <h3><a id="user-content-config" class="anchor" href="https://github.com/groupon/sparklint#config" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Config</h3> 
  <ul> 
   <li>Common config 
    <ul> 
     <li>Set the port of the UI (eg, 4242) 
      <ul> 
       <li>In live mode, send <code>--conf sparklint.port=4242</code> to spark submit script</li> 
       <li>In server mode, send <code>--port 4242</code> to sbt run commandline argument</li> 
      </ul> </li> 
    </ul> </li> 
   <li>Server only config 
    <ul> 
     <li><code>-f [FileName]</code>: Filename of an Spark event log source to use.</li> 
     <li><code>-d [DirectoryName]</code>: Directory of an Spark event log sources to use. Read in filename sort order.</li> 
     <li><code>-p [pollRate]</code>: The interval (in seconds) between polling for changes in directory and history event sources.</li> 
     <li><code>-r</code>: Set the flag in order to run each buffer through to their end state on startup.</li> 
    </ul> </li> 
  </ul> 
  <h3><a id="user-content-developer-cheatsheet" class="anchor" href="https://github.com/groupon/sparklint#developer-cheatsheet" aria-hidden="true" target="_blank">
    <svg aria-hidden="true" class="octicon octicon-link" height="16" version="1.1" viewbox="0 0 16 16" width="16">
     <path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path>
    </svg></a>Developer cheatsheet</h3> 
  <ul> 
   <li>First enter sbt console <code>sbt</code></li> 
   <li>Test: <code>test</code></li> 
   <li>Cross Scala version test <code>+ test</code></li> 
   <li>Rerun failed tests: <code>testQuick</code></li> 
   <li>Change spark version: <code>set sparkVersion := "2.0.0"</code></li> 
   <li>Change scala version: <code>++ 2.11.8</code></li> 
   <li>Package: <code>package</code></li> 
   <li>Perform task (e.g, test) foreach spark version <code>+ foreachSparkVersion test</code></li> 
   <li>Publish to sonatype staging <code>+ foreachSparkVersion publishSigned</code></li> 
   <li>Build docker image to local <code>docker</code> 
    <ul> 
     <li>Snapshot version will be tagged as latest</li> 
     <li>Release version will be tagged as the version number</li> 
    </ul> </li> 
   <li>Publish existing docker image <code>dockerPublish</code></li> 
   <li>Build and publish docker image at the same time <code>dockerBuildAndPublish</code></li> 
   <li>The command to release everything: <code>sparklintRelease</code></li> 
  </ul> 
 </article>
</div>